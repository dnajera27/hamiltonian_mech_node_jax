# -*- coding: utf-8 -*-
"""noAnalyt_noAE_2DOF_UnforcedCubicDamped.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5nAXlX4GwUSv69wQpohT1v-JdybhDg4
"""

!pip install nodepy
!pip install --upgrade "jax[cuda]" -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux.
!pip install -U dm-haiku
!pip install flax

import os
import time
import numpy as np
import scipy as sp
import scipy.io as sio
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy.linalg as la
import glob
import nodepy.linear_multistep_method as lm
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random
import tensorflow as tf
import tensorflow_datasets as tfds
import haiku as hk
import jax
import flax.linen as nn
from flax.training import train_state  # Useful dataclass to keep train state
from typing import Sequence
import optax       
from jax import jacfwd, jacrev

import jax
jax.__version__

from google.colab import drive
import os
drive.flush_and_unmount()
drive.mount('/content/drive')

import seaborn as sns
from matplotlib import rc

def set_style_sns():
    sns.set_context('paper')
    sns.set(font = 'serif')
    sns.set(font_scale = 1.3)
    
    sns.set_style('white', {
        'font.family': 'serif',
        'font.serif': ['Time', 'Palatino', 'serif'],
        'lines.markersize': 10
    })
    
    
plt.rcParams.update({'font.size':16})
set_style_sns()

plt.rcParams.update({'text.usetex': False})

def oscillator(w,t,p):
    x1,x2,xdot1,xdot2 = w
    m1,m2,k1,k2,k_prime,beta1,beta2,beta_prime,F0,omg = p
    
    xdotdot1 = (F0*np.cos(omg*t) + k_prime*(x2 - x1)**3 +  beta_prime*(xdot2 - xdot1)**3 - k1*x1 - beta1*xdot1)/m1
    xdotdot2 = (-k2*x2 + k_prime*(x1 - x2)**3 + beta_prime*(xdot1 - xdot2)**3 - beta2*xdot2)/m2
    
    f = [xdot1, xdot2, xdotdot1, xdotdot2]
    
    return f

def generate_train_data(p, t, w0):
    wsol = sp.integrate.odeint(oscillator, w0, t, args = (p,))
    
    xdotdotsol = np.array(oscillator(wsol.T, t, p)).T
    
    xsol1 = wsol[:,0]
    xdotsol1 = wsol[:,2]
    xdotdotsol1 = xdotdotsol[:,2]
    xsol2 = wsol[:,1]
    xdotsol2 = wsol[:,3]
    xdotdotsol2 = xdotdotsol[:,3]
    
    return xsol1,xdotsol1,xdotdotsol1,xsol2,xdotsol2,xdotdotsol2

def assemble_data(p, t, w0):
  m1,m2,k1,k2,k_prime,beta1,beta2,beta_prime,F0,omg = p
  xsol1,xdotsol1,xdotdotsol1,xsol2,xdotsol2,xdotdotsol2 = generate_train_data(p, t, w0)
  xsol = np.concatenate([xsol1[:,np.newaxis,np.newaxis], xsol2[:,np.newaxis,np.newaxis]], axis = 1)
  xdotsol = np.concatenate([xdotsol1[:,np.newaxis,np.newaxis], xdotsol2[:,np.newaxis,np.newaxis]], axis = 1)
  xdotdotsol = np.concatenate([xdotdotsol1[:,np.newaxis,np.newaxis], xdotdotsol2[:,np.newaxis,np.newaxis]], axis = 1)
  Fext = np.concatenate([F0*np.cos(omg*t).reshape(xsol1.shape)[:,np.newaxis,np.newaxis], np.zeros(xsol1.shape)[:,np.newaxis,np.newaxis]], axis = 1)
  normTrainData = np.concatenate([xsol, xdotsol, xdotdotsol,Fext], axis = 2).reshape((-1,nDof,4))

  return normTrainData

w0 = [0.1,-0.3, 0.4, -0.2]
m1 = 1.1
m2 = 1.5
k1 = 1
k2 = 1.3
k_prime = 10
beta1 = 0.05
beta2 = 0.02
beta_prime = 0.095
F0 = 0.0
omg = 1.5

t = np.linspace(0,100,10000)

p = [m1,m2,k1,k2,k_prime,beta1,beta2,beta_prime,F0,omg]

xsol1,xdotsol1,xdotdotsol1,xsol2,xdotsol2,xdotdotsol2 = generate_train_data(p, t, w0)

dt = t[1] - t[0]

plt.figure(figsize = (10,10))

plt.subplot(3, 1, 1)
plt.plot(t, xsol1)
plt.plot(t, xsol2)

plt.subplot(3, 1, 2)
plt.plot(t, xdotsol1)
plt.plot(t, xdotsol2)

plt.subplot(3, 1, 3)
plt.plot(t, xdotdotsol1)
plt.plot(t, xdotdotsol2)

plt.figure()

xsol = np.concatenate([xsol1[:,np.newaxis,np.newaxis], xsol2[:,np.newaxis,np.newaxis]], axis = 1)
xdotsol = np.concatenate([xdotsol1[:,np.newaxis,np.newaxis], xdotsol2[:,np.newaxis,np.newaxis]], axis = 1)
xdotdotsol = np.concatenate([xdotdotsol1[:,np.newaxis,np.newaxis], xdotdotsol2[:,np.newaxis,np.newaxis]], axis = 1)
Fext = np.zeros(xsol.shape)

nDof = 2

subset = 4000
ixend = 5000
# Choose training data - using linear
normTrainData = np.concatenate([xsol, xdotsol, xdotdotsol,Fext], axis = 2).reshape((-1,nDof,4))
normTrainData = normTrainData[:subset, :]
train_dataset = tf.data.Dataset.from_tensor_slices(normTrainData)
#train_dataset = iter(tfds.as_numpy(train_dataset.batch(128)))

normTestData =  np.concatenate([xsol, xdotsol, xdotdotsol,Fext], axis = 2).reshape((-1,nDof,4))
normTestData = normTestData[subset:ixend, :]
test_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(normTestData,dtype=tf.float32))
#test_dataset = iter(tfds.as_numpy(test_dataset.batch(128)))

uva_max = np.max(np.abs([xsol, xdotsol, xdotdotsol]))
fmax = np.max(np.abs(Fext))
print(uva_max, fmax)
xmax = np.array([uva_max, uva_max, uva_max, 1.0])
print(xmax)

class MLP(nn.Module):
  features: Sequence[int]

  @nn.compact
  def __call__(self, xx):
    for feat in self.features[:-1]:
      xx = nn.tanh(nn.Dense(feat, kernel_init=nn.initializers.glorot_uniform())(xx))
      #xx = nn.Dropout(0.3)(xx, deterministic=DETERMINISTIC)
    xx = nn.Dense(self.features[-1], use_bias = True)(xx)
    return xx

class CNN(nn.Module):
  features = [2, 10, 10, 10, 1]

  @nn.compact
  def __call__(self, xx):
    for feat in self.features[:-1]:
      xx = nn.Conv(features=feat, kernel_size = (2,2), padding = 'same')(xx)
      xx = nn.swish(xx)
    xx = nn.Conv(features=self.features[-1], kernel_size = (2,2), padding = 'same')(xx)

    return xx

class CNN1D(nn.Module):
  features: Sequence[int]

  @nn.compact
  def __call__(self, xx):
    x = xx[0,:LATENT,:]
    dx = xx[0,LATENT:,:]
    
    if self.features[0] == 0:
        xx = x
        xx = jnp.reshape(xx, ((1, LATENT, 1)))
    elif self.features[0] == 1:
        xx = dx
        xx = jnp.reshape(xx, ((1, LATENT, 1)))

    xx_poly = jnp.reshape(jnp.power(jnp.reshape(xx, ((-1,1))), jnp.array([1,2])), LATENT*2)
    xx = nn.Conv(features=4, kernel_size = (1,2), padding = 'valid', use_bias = False)(xx)
    xx = polypool(xx)

    xx = jnp.concatenate([xx_poly, xx]) #skip connection 
    xx = nn.Dense(1, use_bias = True)(xx)

    return xx

class MLP_struct(nn.Module):
  features: Sequence[int]

  @nn.compact
  def __call__(self, xx):
    x = xx[0,:LATENT,:]
    dx = xx[0,LATENT:,:]


    term1 = x[0]**2
    term2 = x[1]**2
    term3 = (x[1] - x[0])**2
    
    term4 = dx[0]**2
    term5 = dx[1]**2
    term6 = (dx[1] - dx[0])**2

    
    
    xvec = jnp.array([term1,term2,term3,term4,term5,term6]).reshape((-1,))

    xvec = xvec[jnp.array(self.features)]

    
    xx = nn.Dense(1, use_bias = False)(xvec)

    return xx

class MLP_poly(nn.Module):
  features: Sequence[int]
  sizes = [2,2,2]

  @nn.compact
  def __call__(self, xx):
    x = xx[0,:LATENT,:]
    dx = xx[0,LATENT:,:]
    
    if self.features[0] == 0:
        xx = x
        xx = jnp.reshape(xx, ((LATENT,)))
        dim = LATENT
    elif self.features[0] == 1:
        xx = dx
        xx = jnp.reshape(xx, ((LATENT,)))
        dim = LATENT
    else:
        xx = jnp.reshape(xx, ((LATENT*2,)))
        dim = LATENT*2
        
    xx_poly = polynomialFeatures(jnp.reshape(xx, ((-1,))), degree = 4, interaction_only = False, include_bias = False).reshape((-1,)) #jnp.reshape(jnp.power(jnp.reshape(xx, ((-1,1))), jnp.array([1,2,3])), dim*3)
    xx_sin = jnp.reshape(jnp.sin(xx), dim)
    xx_cos = jnp.reshape(jnp.cos(xx), dim)
    
    for feat in self.sizes[:-1]:
      xx = nn.swish(nn.Dense(feat, kernel_init=nn.initializers.glorot_uniform(), use_bias = False)(xx))

    xx = nn.swish(nn.Dense(self.sizes[-1], use_bias = False, kernel_init=nn.initializers.glorot_uniform())(xx))
    xx = jnp.concatenate([xx_poly, xx]) #skip connection 
    xx = nn.Dense(1, use_bias = True)(xx)    
    
    return xx


def hessian(f, argnums):
    return jacfwd(jacrev(f, argnums = argnums), argnums = argnums)

def polypool(x):
    #x = jnp.sum(x, axis = 1)
    #x = jnp.reshape(x, (5,))
    x = jnp.power(x, jnp.array([1,2,3,4]))
    x = jnp.reshape(x, ((-1,)))
    return x


def polynomialFeatures( X, degree = 2, interaction_only = False, include_bias = True ) :
    features = X
    prev_chunk = X
    indices = list( range( len( X ) ) )

    for d in range( 1, degree ) :
        # Create a new chunk of features for the degree d:
        new_chunk = []
        # Multiply each component with the products from the previous lower degree:
        for i, v in enumerate( X[:-d] if interaction_only else X ) :
            # Store the index where to start multiplying with the current component
            # at the next degree up:
            next_index = len( new_chunk )
            for coef in prev_chunk[indices[i+( 1 if interaction_only else 0 )]:] :
                new_chunk.append( v*coef )
            indices[i] = next_index
        # Extend the feature vector with the new chunk of features from the degree d:
        features = jnp.append( features, jnp.array(new_chunk))
        prev_chunk = new_chunk

    if include_bias :
        features = jnp.insert( features, 0, 1 )

    return features

LATENT = 2
ORIGINAL_DIM = 2
MSTEPS = 2
METHOD = lm.Adams_Bashforth(MSTEPS)
ALPHA = np.float32(-METHOD.alpha[::-1])
BETA = np.float32(METHOD.beta[::-1])
XMAX = np.max(np.max(normTrainData[:,:,:], axis = 0), axis = 0)
DETERMINISTIC = False
print(XMAX.shape)

        
DT = dt

class CanonicalTransformer(nn.Module):
  def setup(self):
    self.E_layers = MLP([12,12,12,LATENT*ORIGINAL_DIM])
    self.latent = LATENT
    self.original = ORIGINAL_DIM

  def __call__(self, x, dx, ddx):
    x = jnp.reshape(x, (1, self.original))
    dx = jnp.reshape(dx, (1, self.original))
    ddx = jnp.reshape(ddx, (1, self.original))
    in_x = jnp.reshape(x, (self.original)) #jnp.reshape(jnp.concatenate([x, dx, ddx], axis = 1), (self.original*3))
    
    T = jnp.reshape(self.E_layers(in_x), (self.latent,self.original))
    Q = jnp.dot(T, 
                jnp.reshape(in_x, self.original))
    Q = jnp.reshape(Q, (self.latent,))

    return Q

  def get_T(self, x, dx, ddx):
    x = jnp.reshape(x, (1, self.original))
    dx = jnp.reshape(dx, (1, self.original))
    ddx = jnp.reshape(ddx, (1, self.original))
    in_x = jnp.reshape(x, (self.original))
    
    T = jnp.reshape(self.E_layers(in_x), (self.latent,self.original)) 
    return T   
    


class InverseCanonicalTransformer(nn.Module):
  def setup(self):
    self.D_layers = MLP([8,8,8,8,8,LATENT*ORIGINAL_DIM])
    self.latent = LATENT
    self.original = ORIGINAL_DIM

  def __call__(self, x, dx, ddx):
    x = jnp.reshape(x, (1, self.latent, 1))
    dx = jnp.reshape(dx, (1, self.latent, 1))
    ddx = jnp.reshape(ddx, (1, self.latent, 1))
    in_x = jnp.reshape(x, (self.latent)) #jnp.reshape(jnp.concatenate([x, dx, ddx], axis = 1), (self.latent*3))
    
    T = jnp.reshape(self.D_layers(in_x), (self.original,self.latent))
    X = jnp.dot(T, 
                jnp.reshape(in_x, self.latent))
    X = jnp.reshape(X, (self.original,))    
    return X

  def get_T(self, x, dx, ddx):
    x = jnp.reshape(x, (1, self.latent, 1))
    dx = jnp.reshape(dx, (1, self.latent, 1))
    ddx = jnp.reshape(ddx, (1, self.latent, 1))
    in_x = jnp.reshape(x, (self.latent))
    
    T = jnp.reshape(self.D_layers(in_x), (self.original,self.latent)) 
    return T   


class Kinetic(nn.Module):
  def setup(self):
    self.T_layers = MLP_poly([1])
    self.latent = LATENT

  def __call__(self, x, dx):
    x = jnp.reshape(x, (1, self.latent, 1))
    dx = jnp.reshape(dx, (1, self.latent, 1))
    in_x = jnp.concatenate([x, dx], axis = 1)
    
    T = jnp.reshape(self.T_layers(in_x), (-1,))
    
    return T



class Potential(nn.Module):
  def setup(self):
    self.V_layers = MLP_poly([0])
    self.latent = LATENT

  def __call__(self, x, dx):
    x = jnp.reshape(x, (1, self.latent, 1))
    dx = jnp.reshape(dx, (1, self.latent, 1))
    in_x = jnp.concatenate([x, dx], axis = 1)
    
    V = jnp.reshape(self.V_layers(in_x), (-1,))
    
    return V

class Rayleigh(nn.Module):
  def setup(self):
    self.R_layers = MLP_poly([1])
    self.latent = LATENT

  def __call__(self, x, dx):
    x = jnp.reshape(x, (1, self.latent, 1))
    dx = jnp.reshape(dx, (1, self.latent, 1))
    in_x = jnp.concatenate([x, dx], axis = 1)
    
    R = jnp.reshape(self.R_layers(in_x), (-1,))
    
    return R


class Minv_CNN(nn.Module):
  def setup(self):
    self.cnn_layers = CNN()
    self.latent = LATENT

  def __call__(self, Mmat):
    Mmat_re = jnp.reshape(Mmat, (self.latent, self.latent, 1))
    Minv_ = jnp.reshape(self.cnn_layers(Mmat_re), (self.latent, self.latent))

    return Minv_

class AEROM(nn.Module):
  def setup(self):
    self.T_layers = Kinetic()
    self.V_layers = Potential()
    self.R_layers = Rayleigh()
    

    self.latent = LATENT

  def __call__(self, x, dx, ddx):

    #qdot = self.encoderNet_dq(x, dx, ddx)
    #qdotdot = self.encoderNet_ddq(x, dx, ddx)
    T = self.T_layers(x,dx)
    V = self.V_layers(x,dx)
    R = self.R_layers(x,dx)

    #dx_ = self.decoderNet_dq(q, qdot, qdotdot)
    #ddx_ = self.decoderNet_ddq(q, qdot, qdotdot)
    return T


def get_quadratic_energy_jnp(K,x):
  xi = x.reshape((1,LATENT))
  #print(K.shape, xi.shape)
  E = 1/2.*jnp.dot(xi, jnp.dot(K, xi.T))

  return jnp.reshape(E, (-1,))


def Lagrangian(params, x, dx):
  T = eval_kinetic(params, x, dx) #Kinetic().apply({'params': params[0]}, x, dx)
  V = eval_potential(params, x, dx) #Potential().apply({'params': params[1]}, x, dx)
  return T-V

def eval_kinetic(params, x, dx):
  T = Kinetic().apply({'params': params[0]}, x, dx)
  #T = get_quadratic_energy_jnp(MMAT, dx) 
  #print(dx.shape)
  #print(MMAT.shape)
  return T

def eval_potential(params, x, dx):
  V = Potential().apply({'params': params[1]}, x, dx)
  return V

def eval_rayleigh(params, x, dx):
  R = Rayleigh().apply({'params': params[2]}, x, dx)
  #R = get_quadratic_energy_jnp(CMAT, dx) 
  return R




def init_params(data):
  rng = random.PRNGKey(0)
  rng, key = random.split(rng)
  pt = Kinetic().init(key, data[:1,:,0], data[:1,:,1])['params']
  pv = Potential().init(key, data[:1,:,0], data[:1,:,1])['params']

  pM = Minv_CNN().init(key, jnp.zeros((1, LATENT, LATENT, 1)))['params']

  return [pt, pv, pM]

def momentum(params, x, dx):
  latent = LATENT
  dx = jnp.reshape(dx, (-1, latent))
  x = jnp.reshape(x, (-1, latent))

  p = jax.jacfwd(Lagrangian, argnums = 2)(params, x, dx)
  p = jnp.reshape(p, (latent,))
  return p 

def Hamiltonian(params, q, qdot, p):
  latent = LATENT
  qdot = jnp.reshape(qdot, (-1, latent))
  q = jnp.reshape(q, (-1, latent))

  L = Lagrangian(params,q,qdot)

  qdotp = jnp.dot(qdot, jnp.reshape(p, (latent, -1)))

  qdotp = jnp.reshape(qdotp, (-1,))

  H = qdotp - L
            
  return H

def canonical(params, q, qdot, p, Qext):
  latent = LATENT
  dH_dp = jax.jacfwd(Hamiltonian, argnums = 3)(params, q, qdot, p)
  dH_dq = jax.jacfwd(Hamiltonian, argnums = 1)(params, q, qdot, p)
  dR_dqdot = jax.jacfwd(eval_rayleigh, argnums = 2)(params, q, qdot)
  Qext = jnp.reshape(Qext, ((latent,)))
     
  dq_dt = (dH_dp).reshape((latent,))
  dp_dt = (-dH_dq - dR_dqdot + Qext).reshape((latent,))
        
  return [dq_dt, dp_dt]

def EulerLagrange(params, q, qdot, Qext):
  latent = LATENT
  p = momentum(params, q, qdot)

  [_, dp] = canonical(params, q, qdot, p, Qext)

  dL_dqdot = jax.jacfwd(Lagrangian, argnums = 2)(params, q, qdot)  
  dL_dq = jax.jacfwd(Lagrangian, argnums = 1)(params, q, qdot)

  dR_dqdot = jax.jacfwd(eval_rayleigh, argnums = 2)(params, q, qdot)

  d_dL_dqdot_dt = dp #(tape.gradient(L, q) - tape.gradient(R, qdot) + Fext)

  EL_resid = jnp.reshape(d_dL_dqdot_dt, (latent,)) - jnp.reshape(dL_dq, (latent,)) + jnp.reshape(dR_dqdot, (latent,)) - jnp.reshape(Qext, (latent,))

  #dt_check = F_loss(dL_dqdot, d_dL_dqdot_dt, dt = DT)
        
  return jnp.mean(jnp.square(EL_resid)) #+ dt_check*1.0e-3

def get_Mass_Matrix(params, q, dq):
  latent = LATENT
  M = jax.hessian(eval_kinetic, argnums = 2)(params, q, dq)
  M = jnp.reshape(M, (latent, latent))
  return M #jnp.reshape(M, (self.latent, self.latent))

def get_K_Matrix(params, q, dq):
  latent = LATENT
  M = jax.hessian(eval_potential, argnums = 1)(params, q, dq)
  M = jnp.reshape(M, (latent, latent))
  return M #jnp.reshape(M, (self.latent, self.latent))

def get_C_Matrix(params, q, dq):
  latent = LATENT
  M = jax.hessian(eval_rayleigh, argnums = 2)(params, q, dq)
  M = jnp.reshape(M, (latent, latent))
  return M #jnp.reshape(M, (self.latent, self.latent))

def get_dM_dq(params, q, dq):
  dM = jax.jacfwd(get_Mass_Matrix, argnums = 1)(params, q, dq)
  return dM

def get_Minv(params, Mmat):
  latent = LATENT
  Minv = jnp.linalg.pinv(jnp.reshape(Mmat, (latent, latent)))
  return Minv



def get_ddq(params, q, dq, dp):
  latent = LATENT
  Mmat = get_Mass_Matrix(params, q, dq)
  dM = get_dM_dq(params, q, dq)

  Mdq = jnp.matmul(dM, jnp.reshape(dq, (1,latent,1)))

  term2 = jnp.matmul(jnp.reshape(Mdq, (latent,latent)), 
                     jnp.reshape(dq, (latent,1)))

  term1 = jnp.reshape(dp, (latent,))
  term2 = jnp.reshape(term2, (latent,))

  Minv = get_Minv(params, Mmat)

  ddq = jnp.matmul(Minv, jnp.reshape(term1 - term2, (latent, 1)))

  ddq = jnp.reshape(ddq, (latent,))

  return ddq

def general_checks(params, q, dq):

  latent = LATENT 

  Mmat = get_Mass_Matrix(params, q, dq)
  phat = jnp.matmul(jnp.reshape(Mmat, (latent, latent)),
                    jnp.reshape(dq, (latent, 1))
  )
  p = momentum(params, q, dq)
  p_check = jnp.square(jnp.reshape(p, (latent,)) - jnp.reshape(phat, (latent,)))

  ##

  qdotp = jnp.dot(jnp.reshape(dq, (1, latent)), 
                  jnp.reshape(p, (latent, 1))
  )

  That = 1/2.*qdotp 
  T = eval_kinetic(params, q, dq)
  V = eval_potential(params, q, dq)
  T_check = jnp.square(jnp.reshape(T, (-1,)) - jnp.reshape(That, (-1,)))

  ##

  H = Hamiltonian(params, q, dq, p)
  Hhat = T + V
  H_check = jnp.square(H - Hhat)

  ##

  #Minv = get_Minv(params, Mmat)
  #eye_hat = jnp.matmul(jnp.reshape(Mmat, (latent, latent)), 
  #                     jnp.reshape(Minv, (latent, latent))
  #)
  #eye = jnp.eye(latent)

  #Minv_check = jnp.sum(jnp.square(
  #    eye_hat - eye 
  #))

  ##

  Veq = eval_potential(params, jnp.zeros(q.shape), jnp.zeros(dq.shape))

  Veq_check = jnp.square(Veq)

  ##

  neg_M = jnp.mean(jnp.square(jax.nn.relu(jnp.negative(Mmat))))
  neg_T = jnp.mean(jnp.square(jax.nn.relu(jnp.negative(T))))
  neg_V = jnp.mean(jnp.square(jax.nn.relu(jnp.negative(V))))

  neg_checks = neg_M + neg_T + neg_V

  Mnorm = jnp.linalg.norm(Mmat)
  Mnorm_loss = jnp.square(Mnorm.reshape((1,)) - np.linalg.norm(np.array([[1.1,0],[0,1.1]]))/Mnorm)

  ## 
  ix_off =  ~np.eye(Mmat.shape[0],dtype=bool)
  offM = jnp.sum(jnp.square(Mmat[ix_off]))

  ## 
    
  M11 = jnp.square(Mmat[0,0] - 1.0)

  ## 
  #[-0.1,0.5, 0.1, 0]
  #x0 = jnp.array(w0[:LATENT])
  #dx0 = jnp.array(w0[LATENT:])

  #L0 = Lagrangian(params, x0, dx0)
    
  #L0_check = jnp.mean(jnp.square(L0 - L0_analyt))

  return jnp.mean(T_check)*10 + jnp.mean(p_check)*10 + jnp.mean(H_check)*10 #+  L0_check*0.1 #+ neg_checks*1 #+ H_check*0.1 #+ jnp.mean(M11)# + offM #+ Mnorm_loss


def F_loss(p, dp, dt):
  alpha = ALPHA
  beta = BETA
        
  M = MSTEPS
  #dt = DT

  p = jnp.reshape(p, (-1, LATENT))
  dp = jnp.reshape(dp, (-1, LATENT))

  pmax = jnp.max(jnp.array([jnp.max(jnp.abs(p)), 1])) # trying to normalize so amplitudes are better
  p = p/pmax
  dp = dp/pmax
        
  Y = alpha[0]*p[M:, :] + dt*beta[0]*dp[M:, :]
        
  for m in range(1, M+1):
    Y = Y + alpha[m]*p[M-m:-m, :] + dt*beta[m]*dp[M-m:-m,:]
            
  return jnp.mean(jnp.square(Y))


def jnp_cov(x):
  nDof = LATENT
  mean_x = jnp.reshape(jnp.mean(x, axis = 0), (1, nDof))
  mx = jnp.matmul(mean_x.T, mean_x)
  vx = jnp.matmul(x.T, x) #/tf.cast(tf.shape(x)[0], tf.float32)
  cov_xx = vx - mx
  return cov_xx

def l2_loss(x, alpha):
    return alpha * (x ** 2).mean()

def l1_loss(x, alpha):
  return alpha * jnp.abs(x).sum()

@jax.jit
def f_diff(params, x, dx, Fext):
  latent = LATENT

  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  p = jax.vmap(momentum, in_axes = (None, 0, 0))(params_, x, dx)
  [dq,dp] = jax.vmap(canonical, in_axes = (None, 0, 0, 0, 0))(params_, x, dx, p, Fext)
  ddq = jax.vmap(get_ddq, in_axes = (None, 0, 0, 0))(params_, x, dx, dp)

  X = jnp.concatenate([jnp.reshape(dx, (-1, latent)), 
                       jnp.reshape(ddq, (-1, latent))], axis = -1)

  return X

def check_1(params, batch):
  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  q = batch[:, :, 0]
  qdot = batch[:, :, 1]
  ddq = batch[:, :, 2]
  Qext = batch[:, :, 3]

  p = jax.vmap(momentum, in_axes = (None, 0, 0))(params_, q, qdot)

  [dq, dp] = jax.vmap(canonical, in_axes = (None, 0, 0, 0, 0))(params_, q, qdot, p, Qext)

  dL_dqdot = jax.vmap(jax.jacfwd(Lagrangian, argnums = 2), in_axes = (None, 0, 0))(params_, q, qdot)  
  dL_dq = jax.vmap(jax.jacfwd(Lagrangian, argnums = 1), in_axes = (None, 0, 0))(params_, q, qdot)

  d_dL_dqdot_dt = dp #(tape.gradient(L, q) - tape.gradient(R, qdot) + Fext)

  return [p,dp,dL_dqdot,dL_dq,dq]




def predict_ddq(params, batch):
  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  x = batch[:, :, 0]
  dx = batch[:, :, 1]
  ddx = batch[:, :, 2]
  Qext = batch[:, :, 3]

  p = jax.vmap(momentum, in_axes = (None, 0, 0))(params_, x, dx)
  [_,dp] = jax.vmap(canonical, in_axes = (None, 0, 0, 0, 0))(params_, x, dx, p, Qext)
  ddq = jax.vmap(get_ddq, in_axes = (None, 0, 0, 0))(params_, x, dx, dp)

  return ddq

def predict_kinetic(params, batch):
  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  x = batch[:, :, 0]
  dx = batch[:, :, 1]
  ddx = batch[:, :, 2]

  T = jax.vmap(eval_kinetic, in_axes = (None, 0, 0))(params_, x, dx)

  return T

def predict_potential(params, batch):
  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  x = batch[:, :, 0]
  dx = batch[:, :, 1]
  ddx = batch[:, :, 2]

  V = jax.vmap(eval_potential, in_axes = (None, 0, 0))(params_, x, dx)

  return V

def predict_mass(params, batch):
  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  x = batch[:, :, 0]
  dx = batch[:, :, 1]
  ddx = batch[:, :, 2]

  M = jax.vmap(get_Mass_Matrix, in_axes = (None, 0, 0))(params_, x, dx)
  Minv = jax.vmap(get_Minv, in_axes = (None, 0))(params_, M)

  return M, Minv

def predict_MKC(params, batch):
  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  x = batch[:, :, 0]
  dx = batch[:, :, 1]
  ddx = batch[:, :, 2]

  M = jax.vmap(get_Mass_Matrix, in_axes = (None, 0, 0))(params_, x, dx)
  K = jax.vmap(get_K_Matrix, in_axes = (None, 0, 0))(params_, x, dx)
  C = jax.vmap(get_C_Matrix, in_axes = (None, 0, 0))(params_, x, dx)

  return M,K,C

def compute_metrics(loss):
  metrics = {
      'loss': loss
  }
  return metrics


def lasso_loss(params, alpha = 1e-3):
  T_params = params['T_layers']
  V_params = params['V_layers']
  wT = Kinetic().apply({'params': T_params}, method = Kinetic().get_coeffs)
  wV = Potential().apply({'params': V_params}, method = Potential().get_coeffs)
    
  return l1_loss(wT, alpha) + l1_loss(wV, alpha) 

def loss_fn_(params, batch):
  T_params = params['T_layers']
  V_params = params['V_layers']
  R_params = params['R_layers']

  params_ = [T_params, V_params, R_params]

  x = batch[:, :, 0]
  dx = batch[:, :, 1]
  ddx = batch[:, :, 2]
  Qext = batch[:, :, 3]

  p = jax.vmap(momentum, in_axes = (None, 0, 0))(params_, x, dx)
  
  
  [dq,dp] = jax.vmap(canonical, in_axes = (None, 0, 0, 0, 0))(params_, x, dx, p, Qext)
  #EL_loss = jax.vmap(EulerLagrange, in_axes = (None, 0, 0, 0))(params_, x, dq, Qext)
  ddq = jax.vmap(get_ddq, in_axes = (None, 0, 0, 0))(params_, x, dq, dp)
  dL_dqdot = jax.vmap(jax.jacfwd(Lagrangian, argnums = 2), in_axes = (None, 0, 0))(params_, x, dq)

  F_loss_dp = F_loss(p, dp, dt = DT)*500
  F_loss_dx = F_loss(dq, ddq, dt = DT)*500  
  F_loss_x = F_loss(x, dq, dt = DT)*500

    
  recon_loss_ddq_enc = jnp.mean(jnp.square((ddx - ddq)/XMAX[2]))

  recon_loss = recon_loss_ddq_enc*10 

  checks_loss = jax.vmap(general_checks, in_axes = (None, 0, 0))(params_, x, dq)

  d_dL_dqdot_dt = dp #(tape.gradient(L, q) - tape.gradient(R, qdot) + Fext)

  EL_check = F_loss(dL_dqdot, d_dL_dqdot_dt, dt = DT) 


  loss = jnp.array([F_loss_dp, F_loss_x, F_loss_dx, recon_loss, jnp.mean(checks_loss)*5, jnp.mean(EL_check)])

  return loss

def loss_fn(params, batch):
  loss = loss_fn_(params, batch)
  loss = jnp.sum(loss)

  loss += sum(
        l2_loss(w, alpha=1.0e-5) 
        for w in jax.tree_leaves(params)
    )

  return loss

@jax.jit
def train_step(state, batch):
  grad_fn = jax.value_and_grad(loss_fn, has_aux=False)
  loss, grads = grad_fn(state.params, batch)
  state = state.apply_gradients(grads=grads)
  metrics = compute_metrics(loss)
  return state, metrics


def train_epoch(state, train_ds, batch_size, epoch, rng):
  """Train for a single epoch."""
  train_ds_size = len(train_ds)
  steps_per_epoch = train_ds_size // batch_size

  train_ds_batched = train_ds.batch(batch_size).prefetch(1)
  train_ds_batched = tfds.as_numpy(train_ds_batched)

  batch_metrics = []
  for batch in train_ds_batched:
    state, metrics = train_step(state, batch)
    batch_metrics.append(metrics)

  # compute mean of metrics across each batch in epoch.
  batch_metrics_np = jax.device_get(batch_metrics)
  epoch_metrics_np = {
      k: np.mean([metrics[k] for metrics in batch_metrics_np])
      for k in batch_metrics_np[0]}

  if epoch % 100 == 0:
    print('train epoch: %d, loss: %.8f' % (
        epoch, epoch_metrics_np['loss']))

  return state


@jax.jit
def eval_step(params, batch):
  loss = loss_fn(params, batch)
  return compute_metrics(loss)

def eval_model(params, test_ds):

  test_ds_batched = test_ds.batch(len(test_ds)).prefetch(1)
  test_ds_batched = tfds.as_numpy(test_ds_batched)

  for batch in test_ds_batched:
    metrics = eval_step(params, batch)
    metrics = jax.device_get(metrics)
    summary = jax.tree_map(lambda x: x.item(), metrics)

  return summary['loss']

rng = random.PRNGKey(0)
rng, key = random.split(rng)
init_rng = {'params': random.PRNGKey(0), 'dropout': random.PRNGKey(1)}
initial_variables = AEROM().init(rng, normTrainData[:1,:,0], normTrainData[:1,:,1], normTrainData[:1,:,0])

state = train_state.TrainState.create(
    apply_fn = AEROM().apply,
    params = initial_variables['params'],
    tx = optax.adam(
          learning_rate=1e-4,
          b1=0.9,
          b2=0.98,
          eps=1e-7)
)

from flax.training import checkpoints
#dir_name = "/content/drive/MyDrive/UCSD_research/jax/oscillator/chkpts_2DOF_UnforcedDamped_noAE_noAnalyt/"
dir_name = "/content/drive/MyDrive/UCSD_research/jax/oscillator/chkpts_2DOF_UnforcedCubeDamped_noAE_noAnalyt/"
load = 0
purge = 0

if load:
    test = os.listdir(dir_name)
    chk_latest = checkpoints.latest_checkpoint(dir_name)
    state = checkpoints.restore_checkpoint(chk_latest, state)

    
if purge:
    test = os.listdir(dir_name)
    for item in test:
          os.remove(os.path.join(dir_name, item))

from flax.training import checkpoints
num_epochs = 50000
batch_size = 128

for epoch in range(1, num_epochs + 1):
  # Use a separate PRNG key to permute image data during shuffling
  rng, input_rng = jax.random.split(rng)
  # Run an optimization step over a training batch
  state = train_epoch(state, train_dataset, batch_size, epoch, input_rng)
  # Evaluate on the test set after each training epoch
  test_loss= eval_model(state.params, test_dataset)
  if epoch % 100 == 0:
    print(' test epoch: %d, loss: %.8f' % (
        epoch, test_loss))
  if epoch % 500 == 0:
    checkpoints.save_checkpoint(dir_name, state, epoch, keep=3)

print(F_loss(normTrainData[:1000,:,0], normTrainData[:1000,:,1], dt= DT))

print(loss_fn_(state.params, normTrainData[:100,:,:]))

Nsim = 1000
ddq = predict_ddq(state.params, normTrainData[:Nsim,:,:])

T = predict_kinetic(state.params, normTrainData[:Nsim,:,:])
V = predict_potential(state.params, normTrainData[:Nsim,:,:])

M,K,C = predict_MKC(state.params, normTrainData[:Nsim,:,:])

#print(M[0])
#print(MMAT)

print(K[10])
#print(Kmat)
print(C[0])
#print(Cmat)

plt.figure(figsize = (14, 6))
plt.subplot(3,1,1)

plt.plot(normTrainData[:Nsim,:,2], linewidth = 2)
plt.plot(ddq[:,:], 'k--')
plt.subplot(3,1,2)
plt.plot(T)
#plt.plot(Tanalyt[:Nsim], 'k--')
plt.subplot(3,1,3)
plt.plot(V)
#plt.plot(Vanalyt[:Nsim], 'k--')

plt.tight_layout()
plt.show()

from scipy.signal import butter, lfilter, freqz
from scipy.interpolate import interp1d 

Qext_pred = normTrainData[:Nsim,:,-1]

time_vec_int = t[:].reshape((-1,))
fbar_int = interp1d(t[:Qext_pred.shape[0]], Qext_pred.T)

qICs = np.concatenate([normTrainData[0,:,0], normTrainData[0,:,1]])

normTrainData.shape

import time
from scipy.interpolate import interp1d 
import scipy.signal 

dof = LATENT

def tf_dydx(q, dq, f, params):
    return f_diff(params, q, dq, f)
    

def learned_q(x, t, params, fbar):
    x = np.array(x)
    t = np.array([t])
    
    dof = LATENT
    
    q = x[:dof]
    dq = x[dof:]
    
    f = fbar(t).reshape((-1,))

    
    q = q.reshape((-1,LATENT))
    dq = dq.reshape((-1,LATENT))
    f = f.reshape((-1,LATENT))
    
    dydx = tf_dydx(q, dq, f, params)
    
    #qqdot = x.reshape((1, dof*2)) #np.concatenate([x, f], axis = 1)
    
    #dydx = model.F_layers_q(qqdot).numpy()
    
    return dydx.flatten()


Nint = 900

import time

start = time.time()
qout = scipy.integrate.odeint(learned_q, qICs, t[:Nint], (state.params,fbar_int))
end = time.time()
print(end - start)

plt.figure(figsize = (10,6))
nDof = 2

xpred = qout[:, :dof]
vpred = qout[:, dof:]

plt.subplot(2, 1, 1)

plt.plot(t[:Nint], normTrainData[:Nint,:,0], linewidth = 4, alpha = 0.6)
plt.plot(t[:Nint], xpred, 'k')
plt.ylabel('Displacement')
plt.xlabel('Time, s')

plt.subplot(2, 1, 2)

plt.plot(t[:Nint], normTrainData[:Nint,:,1], linewidth = 4, alpha = 0.6)
plt.plot(t[:Nint], vpred, 'k')
plt.ylabel('Velocity')
plt.xlabel('Time, s')
#plt.xlim([0, 0.1])

plt.tight_layout()
#plt.savefig('latent_displacements_time_NL.tif')

plt.show()

dir_name_lindamp = "/content/drive/MyDrive/UCSD_research/jax/oscillator/chkpts_2DOF_UnforcedDamped_noAE_noAnalyt/"

chk_latest = checkpoints.latest_checkpoint(dir_name_lindamp)
linear_damped_state = checkpoints.restore_checkpoint(chk_latest, state)

T_weights_lin = linear_damped_state.params['T_layers']['T_layers']['Dense_3']['kernel']
T_weights_cube = state.params['T_layers']['T_layers']['Dense_3']['kernel']

V_weights_lin = linear_damped_state.params['V_layers']['V_layers']['Dense_3']['kernel']
V_weights_cube = state.params['V_layers']['V_layers']['Dense_3']['kernel']

R_weights_lin = linear_damped_state.params['R_layers']['R_layers']['Dense_3']['kernel']
R_weights_cube = state.params['R_layers']['R_layers']['Dense_3']['kernel']

plt.figure(figsize=(22,4))
plt.subplot(2,3,1)
plt.imshow(T_weights_lin.T)
plt.subplot(2,3,2)
plt.imshow(V_weights_lin.T)
plt.subplot(2,3,3)
plt.imshow(R_weights_lin.T)
plt.subplot(2,3,4)
plt.imshow(T_weights_cube.T)
plt.subplot(2,3,5)
plt.imshow(V_weights_cube.T)
plt.subplot(2,3,6)
plt.imshow(R_weights_cube.T)

plt.figure(figsize=(8,4))

for ii,lay in enumerate(['T_layers', 'V_layers', 'R_layers']):
  plt.subplot(1,3,ii+1)
  wts_lin = jax.tree_leaves(linear_damped_state.params[lay])
  wts_list_lin = [wt.reshape((-1,)) for wt in wts_lin]
  wts_flatten_lin = np.concatenate(wts_list_lin).ravel()

  wts_cube = jax.tree_leaves(state.params[lay])
  wts_list_cube = [wt.reshape((-1,)) for wt in wts_cube]
  wts_flatten_cube = np.concatenate(wts_list_cube).ravel()

  plt.hist(wts_flatten_lin - wts_flatten_cube)


plt.tight_layout()
plt.show()

wtest0 = [-0.3, 0.1, -0.1, -0.1]
m1 = 1.1
m2 = 1.5
k1 = 1
k2 = 1.3
k_prime = 10
beta1 = 0.05
beta2 = 0.02
beta_prime = 0.095
F0 = 0.0
omg = 1.5

t = np.linspace(0,100,10000)

ptest = [m1,m2,k1,k2,k_prime,beta1,beta2,beta_prime,F0,omg]

testData = assemble_data(ptest, t, wtest0)

Nint = 2000

Qext_pred_test = testData[:,:,-1]

time_vec_int = t[:].reshape((-1,))
fbar_int_test = interp1d(t[:Qext_pred_test.shape[0]], Qext_pred_test.T)

qICs_test = np.concatenate([testData[0,:,0], testData[0,:,1]])

start = time.time()
qout = scipy.integrate.odeint(learned_q, qICs_test, t[:Nint], (state.params,fbar_int_test))
end = time.time()
print((end - start)/Nint)

plt.figure(figsize = (10,6))
nDof = 2

xpred = qout[:, :dof]
vpred = qout[:, dof:]

plt.subplot(2, 1, 1)

plt.plot(t[:Nint], testData[:Nint,:,0], linewidth = 4, alpha = 0.6)
plt.plot(t[:Nint], xpred, 'k')
plt.ylabel('Displacement')
plt.xlabel('Time, s')

plt.subplot(2, 1, 2)

plt.plot(t[:Nint], testData[:Nint,:,1], linewidth = 4, alpha = 0.6)
plt.plot(t[:Nint], vpred, 'k')
plt.ylabel('Velocity')
plt.xlabel('Time, s')
#plt.xlim([0, 0.1])

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/UCSD_research/jax/oscillator/figures/2DOF_UnforcedCubedDamped_time.png')

plt.show()

mse = np.mean((testData[:Nint,:,0] - xpred)**2)
print(mse)

plt.figure(figsize = (6,6))

plt.plot(testData[:Nint,:,0], testData[:Nint,:,1], linewidth = 4, alpha = 0.6)
plt.plot(xpred, vpred, 'k')

plt.ylabel('Velocity')
plt.xlabel('Displacement')

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/UCSD_research/jax/oscillator/figures/2DOF_UnforcedCubedDamped_phase.png')
plt.show()

wtest0 = [0.1, 0.3, -0.1, 0.1]
m1 = 1.1
m2 = 1.5
k1 = 1
k2 = 1.3
k_prime = 10
beta1 = 0.05
beta2 = 0.02
F0 = 0.1
omg = 1.5

t = np.linspace(0,100,10000)

ptest = [m1,m2,k1,k2,k_prime,beta1,beta2,F0,omg]

testData = assemble_data(ptest, t, wtest0)

Nint = 3000

Qext_pred_test = testData[:,:,-1]

time_vec_int = t[:].reshape((-1,))
fbar_int_test = interp1d(t[:Qext_pred_test.shape[0]], Qext_pred_test.T)

qICs_test = np.concatenate([testData[0,:,0], testData[0,:,1]])

start = time.time()
qout = scipy.integrate.odeint(learned_q, qICs_test, t[:Nint], (state.params,fbar_int_test))
end = time.time()
print((end - start)/Nint)

plt.figure(figsize = (10,6))
nDof = 2

xpred = qout[:, :dof]
vpred = qout[:, dof:]

plt.subplot(2, 1, 1)

plt.plot(t[:Nint], testData[:Nint,:,0], linewidth = 4, alpha = 0.6)
plt.plot(t[:Nint], xpred, 'k')
plt.ylabel('Displacement')
plt.xlabel('Time, s')

plt.subplot(2, 1, 2)

plt.plot(t[:Nint], testData[:Nint,:,1], linewidth = 4, alpha = 0.6)
plt.plot(t[:Nint], vpred, 'k')
plt.ylabel('Velocity')
plt.xlabel('Time, s')
#plt.xlim([0, 0.1])

plt.tight_layout()
#plt.savefig('latent_displacements_time_NL.tif')

plt.show()

MAC = np.zeros((LATENT, LATENT))


for ii in range(LATENT):
    for jj in range(LATENT):
        MAC[ii,jj] = np.dot(d[:,ii], dnn[:,jj])**2/(np.dot(d[:,ii], d[:,ii])*np.dot(dnn[:,jj], dnn[:,jj]))

plt.figure(figsize = (14,6))
plt.subplot(1,2,1)
plt.imshow(MAC, cmap = 'binary', vmin = 0, vmax = 1)
plt.colorbar()
plt.title('MAC')
plt.axis('off')
plt.subplot(1,2,2)
plt.imshow(cross_ortho, cmap = 'binary', vmin = 0, vmax = 1)
plt.colorbar()
plt.axis('off')
plt.title('Cross-Orthogonality Check')
plt.show()

mr = []
for ii in range(LATENT):
  mr.append(np.dot(d[:,ii], np.dot(Mmat, d[:,ii].T)))

mr = np.array(mr)
dnorm = d/np.sqrt(mr)
dnn_norm = dnn/np.sqrt(mr)

cross_ortho = np.dot(dnorm.T, np.dot(Mmat, dnn_norm))

